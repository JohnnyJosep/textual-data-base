{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploració de dades\n",
    "\n",
    "Carregar dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/data.pickle', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Señorías, antes de nada quiero recordar a quienes ocupan las tribunas de invitados que no pueden hacer manifestaciones de ningún tipo mientras dura la sesión. Se abre la sesión con el único punto en el orden del día, como ustedes saben, relativo al debate sobre la investidura del candidato a la Presidencia del Gobierno. Para ello, por la secretaria primera de la Cámara se va a proceder a la lectura de la propuesta de candidato a la Presidencia del Gobierno.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/spanish.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\JORDI/nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JORDI\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-61e518b08b5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msentenceTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-61e518b08b5a>\u001b[0m in \u001b[0;36msentenceTokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msentenceTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mspanish_sentence_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/spanish.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspanish_sentence_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\sources\\tfg\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\sources\\tfg\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\sources\\tfg\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/spanish.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\JORDI/nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\sources\\\\tfg\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\JORDI\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def sentenceTokenizer(text):\n",
    "    spanish_sentence_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "    sentences = spanish_sentence_tokenizer.tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "sentenceTokenizer(data[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words\n",
    "\n",
    "Usando Freeling para asi tambien obtener el lema de cada palabra\n",
    "```\n",
    "docker build -t freeling ./docker/freeling\n",
    "docker run -it --rm --name freeling -p 50005:50005 freeling analyze -f es.cfg --server -p 50005\n",
    "```\n",
    "\n",
    "PoS Tag: https://freeling-user-manual.readthedocs.io/en/v4.2/tagsets/tagset-es/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Señorías señoría NCFP000 1',\n",
       " ', , Fc 1',\n",
       " 'antes_de antes_de SP 1',\n",
       " 'nada nada PI0CS00 0.893578',\n",
       " 'quiero querer VMIP1S0 1',\n",
       " 'recordar recordar VMN0000 1',\n",
       " 'a a SP 0.998775',\n",
       " 'quienes quien PR0CP00 1',\n",
       " 'ocupan ocupar VMIP3P0 1',\n",
       " 'las el DA0FP0 0.988184',\n",
       " 'tribunas tribuna NCFP000 1',\n",
       " 'de de SP 0.999961',\n",
       " 'invitados invitado NCMP000 0.822581',\n",
       " 'que que PR0CN00 0.550139',\n",
       " 'no no RN 0.999297',\n",
       " 'pueden poder VMIP3P0 1',\n",
       " 'hacer hacer VMN0000 1',\n",
       " 'manifestaciones manifestación NCFP000 1',\n",
       " 'de de SP 0.999961',\n",
       " 'ningún ninguno DI0MS0 1',\n",
       " 'tipo tipo NCMS000 1',\n",
       " 'mientras mientras CS 0.88835',\n",
       " 'dura durar VMIP3S0 0.157895',\n",
       " 'la el DA0FS0 0.98926',\n",
       " 'sesión sesión NCFS000 1',\n",
       " '. . Fp 1']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "def getRandomString(length):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n",
    "\n",
    "\n",
    "class FreelingResult:\n",
    "    def __init__(self, line):\n",
    "        parts = line.split()\n",
    "        self.word = parts[0]\n",
    "        self.lema = parts[1]\n",
    "        self.postag = parts[2]\n",
    "        self.probability = parts[3]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.word} {self.lema} {self.postag} {self.probability}'\n",
    "        \n",
    "        \n",
    "class Freeling:\n",
    "    def __init__(self, port = 50005):\n",
    "        self.port = port\n",
    "            \n",
    "    def analyzer(self, text):\n",
    "        temp = f'{getRandomString(10)}.txt'\n",
    "        with open(temp, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        command = f'analyzer_client.exe {self.port} < {temp}'\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n",
    "        out, _ = process.communicate()\n",
    "        os.remove(temp)\n",
    "        out = out.decode('utf-8').strip()\n",
    "        lines = out.splitlines()\n",
    "        return [FreelingResult(line) for line in lines if line != \"\"]\n",
    "        \n",
    "freeling = Freeling()\n",
    "words = freeling.analyzer(sentences[0])\n",
    "[str(f) for f in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Señorías',\n",
       " ',',\n",
       " 'antes_de',\n",
       " 'nada',\n",
       " 'quiero',\n",
       " 'recordar',\n",
       " 'a',\n",
       " 'quienes',\n",
       " 'ocupan',\n",
       " 'las',\n",
       " 'tribunas',\n",
       " 'de',\n",
       " 'invitados',\n",
       " 'que',\n",
       " 'no',\n",
       " 'pueden',\n",
       " 'hacer',\n",
       " 'manifestaciones',\n",
       " 'de',\n",
       " 'ningún',\n",
       " 'tipo',\n",
       " 'mientras',\n",
       " 'dura',\n",
       " 'la',\n",
       " 'sesión',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTokens(freelingResult):\n",
    "    return [f.word for f in freelingResult]\n",
    "\n",
    "tokens = getTokens(words)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Del vector de paraules anterior anam a filtrar aquelles paraulres que no tenen un significat substancial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "postagConjunction = 'C'\n",
    "postagDeterminer = 'D'\n",
    "postagPronoun = 'P'\n",
    "postagAdposition = 'S'\n",
    "postagPunctuation = 'F'\n",
    "\n",
    "def filterStopwordsViaPosTagCategory(freelingResult, postTagsCategoryExcluded = [postagConjunction, postagDeterminer, postagPronoun, postagAdposition]):\n",
    "    filtered = filter(lambda f : not f.postag[0] in postTagsCategoryExcluded, freelingResult)\n",
    "    return list(filtered)\n",
    "\n",
    "def containsSomeElement(sublist, list):\n",
    "    for e in sublist:\n",
    "        if e in list:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filterStopwordsViaList(freelingResult, stopwords = spanish_stopwords):\n",
    "    filtered = filter(lambda f: not containsSomeElement(f.word.split('_'), stopwords), freelingResult)\n",
    "    return list(filtered)\n",
    "\n",
    "def filterStopwords(freelingResult, postTagsCategoryExcluded = [postagConjunction, postagDeterminer, postagPronoun, postagAdposition], stopwords = spanish_stopwords):\n",
    "    filtered = filterStopwordsViaPosTagCategory(freelingResult, postTagsCategoryExcluded)\n",
    "    filtered = filterStopwordsViaList(filtered, stopwords)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Señorías señoría NCFP000 1',\n",
       " ', , Fc 1',\n",
       " 'quiero querer VMIP1S0 1',\n",
       " 'recordar recordar VMN0000 1',\n",
       " 'ocupan ocupar VMIP3P0 1',\n",
       " 'tribunas tribuna NCFP000 1',\n",
       " 'invitados invitado NCMP000 0.822581',\n",
       " 'no no RN 0.999297',\n",
       " 'pueden poder VMIP3P0 1',\n",
       " 'hacer hacer VMN0000 1',\n",
       " 'manifestaciones manifestación NCFP000 1',\n",
       " 'tipo tipo NCMS000 1',\n",
       " 'dura durar VMIP3S0 0.157895',\n",
       " 'sesión sesión NCFS000 1',\n",
       " '. . Fp 1']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFilteredSTPostTag = filterStopwordsViaPosTagCategory(words)\n",
    "[str(f) for f in wordsFilteredSTPostTag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Señorías señoría NCFP000 1',\n",
       " ', , Fc 1',\n",
       " 'quiero querer VMIP1S0 1',\n",
       " 'recordar recordar VMN0000 1',\n",
       " 'ocupan ocupar VMIP3P0 1',\n",
       " 'tribunas tribuna NCFP000 1',\n",
       " 'invitados invitado NCMP000 0.822581',\n",
       " 'pueden poder VMIP3P0 1',\n",
       " 'hacer hacer VMN0000 1',\n",
       " 'manifestaciones manifestación NCFP000 1',\n",
       " 'ningún ninguno DI0MS0 1',\n",
       " 'tipo tipo NCMS000 1',\n",
       " 'mientras mientras CS 0.88835',\n",
       " 'dura durar VMIP3S0 0.157895',\n",
       " 'sesión sesión NCFS000 1',\n",
       " '. . Fp 1']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFilteredSTList = filterStopwordsViaList(words)\n",
    "[str(f) for f in wordsFilteredSTList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Señorías señoría NCFP000 1',\n",
       " ', , Fc 1',\n",
       " 'quiero querer VMIP1S0 1',\n",
       " 'recordar recordar VMN0000 1',\n",
       " 'ocupan ocupar VMIP3P0 1',\n",
       " 'tribunas tribuna NCFP000 1',\n",
       " 'invitados invitado NCMP000 0.822581',\n",
       " 'pueden poder VMIP3P0 1',\n",
       " 'hacer hacer VMN0000 1',\n",
       " 'manifestaciones manifestación NCFP000 1',\n",
       " 'tipo tipo NCMS000 1',\n",
       " 'dura durar VMIP3S0 0.157895',\n",
       " 'sesión sesión NCFS000 1',\n",
       " '. . Fp 1']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFilteredST = filterStopwords(words)\n",
    "[str(f) for f in wordsFilteredST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abrasador', 'acogedor', 'afectivo', 'afectuoso', 'amable',\n",
       "       'amigable', 'ardiente', 'ardoroso', 'caliente', 'calinoso',\n",
       "       'caluroso', 'candente', 'canicular', 'cordial', 'cálido',\n",
       "       'entrañable', 'moderado', 'sofocante', 'suave', 'templado',\n",
       "       'tropical', 'tórrido'], dtype='<U10')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import WordNetCorpusReader\n",
    "import numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import unidecode\n",
    "\n",
    "wncr = WordNetCorpusReader('./wordnet_spa', None)\n",
    "cache = {}\n",
    "\n",
    "def getSynonyms(word):\n",
    "    wn_synonyms = [ss.name().split('.')[0] for ss in wncr.synsets(word)]\n",
    "    \n",
    "    eduacalingo_synonyms = []\n",
    "    try:\n",
    "        data = urllib.request.urlopen(f'https://educalingo.com/en/dic-es/{unidecode.unidecode(word)}').read()\n",
    "        eduacalingo_synonyms = [s.text for s in soup(data, 'html.parser').select('.contenido_sinonimos_antonimos0')[0].select('a')]\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    synonyms = np.unique(wn_synonyms + eduacalingo_synonyms)\n",
    "    return synonyms\n",
    "\n",
    "def getCacheSynonyms(word):\n",
    "    word = word.lower()\n",
    "    if word in cache:\n",
    "        return cache[word]\n",
    "    else:\n",
    "        synonyms = getSynonyms(word)\n",
    "        cache[word] = synonyms\n",
    "        return synonyms\n",
    "\n",
    "getCacheSynonyms('cálido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Señorías', ','],\n",
       " [',', 'antes_de'],\n",
       " ['antes_de', 'nada'],\n",
       " ['nada', 'quiero'],\n",
       " ['quiero', 'recordar'],\n",
       " ['recordar', 'a'],\n",
       " ['a', 'quienes'],\n",
       " ['quienes', 'ocupan'],\n",
       " ['ocupan', 'las'],\n",
       " ['las', 'tribunas'],\n",
       " ['tribunas', 'de'],\n",
       " ['de', 'invitados'],\n",
       " ['invitados', 'que'],\n",
       " ['que', 'no'],\n",
       " ['no', 'pueden'],\n",
       " ['pueden', 'hacer'],\n",
       " ['hacer', 'manifestaciones'],\n",
       " ['manifestaciones', 'de'],\n",
       " ['de', 'ningún'],\n",
       " ['ningún', 'tipo'],\n",
       " ['tipo', 'mientras'],\n",
       " ['mientras', 'dura'],\n",
       " ['dura', 'la'],\n",
       " ['la', 'sesión'],\n",
       " ['sesión', '.']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def getNGrams(freelingResult, size = 2):\n",
    "    listNGrams = list(ngrams(freelingResult, size))\n",
    "    return listNGrams\n",
    "\n",
    "twoGrams = getNGrams(words)\n",
    "[[f[0].word, f[1].word] for f in twoGrams]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dels *2-grames* anteriors podem notar la presencia de signes de puntuació, per la propia naturalesa d'aquets separen unitats d'informació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['antes_de', 'nada'],\n",
       " ['nada', 'quiero'],\n",
       " ['quiero', 'recordar'],\n",
       " ['recordar', 'a'],\n",
       " ['a', 'quienes'],\n",
       " ['quienes', 'ocupan'],\n",
       " ['ocupan', 'las'],\n",
       " ['las', 'tribunas'],\n",
       " ['tribunas', 'de'],\n",
       " ['de', 'invitados'],\n",
       " ['invitados', 'que'],\n",
       " ['que', 'no'],\n",
       " ['no', 'pueden'],\n",
       " ['pueden', 'hacer'],\n",
       " ['hacer', 'manifestaciones'],\n",
       " ['manifestaciones', 'de'],\n",
       " ['de', 'ningún'],\n",
       " ['ningún', 'tipo'],\n",
       " ['tipo', 'mientras'],\n",
       " ['mientras', 'dura'],\n",
       " ['dura', 'la'],\n",
       " ['la', 'sesión']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def containsPunctuationMarks(ngramFreeLing):\n",
    "    for ng in ngramFreeLing:\n",
    "        if ng.postag.startswith('F'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNGramsFilteringPunctuationMarks(freelingResult, size = 2):\n",
    "    listNGrams = getNGrams(freelingResult, size)    \n",
    "    filtered = filter(lambda ng: not containsPunctuationMarks(ng), listNGrams)\n",
    "    return filtered\n",
    "    \n",
    "    \n",
    "filteredTwoGrams = getNGramsFilteringPunctuationMarks(words)\n",
    "[[f[0].word, f[1].word] for f in filteredTwoGrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['querer', 'recordar', 'ocupar'],\n",
       " ['recordar', 'ocupar', 'tribuna'],\n",
       " ['ocupar', 'tribuna', 'invitado'],\n",
       " ['tribuna', 'invitado', 'poder'],\n",
       " ['invitado', 'poder', 'hacer'],\n",
       " ['poder', 'hacer', 'manifestación'],\n",
       " ['hacer', 'manifestación', 'tipo'],\n",
       " ['manifestación', 'tipo', 'durar'],\n",
       " ['tipo', 'durar', 'sesión']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredThreeGrams = getNGramsFilteringPunctuationMarks(wordsFilteredST, size=3)\n",
    "[[f[0].lema, f[1].lema, f[2].lema] for f in filteredThreeGrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "debateTitle = data[0].meta['debate']\n",
    "debate = [d for d in data if d.meta['debate'] == debateTitle]\n",
    "\n",
    "freelings = []\n",
    "for d in debate:\n",
    "    d.meta['freeling'] = Freeling().analyzer(d.text)\n",
    "    freelings = freelings + d.meta['freeling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gracias, presidente. De acuerdo con lo establecido en el artículo 99.1 de la Constitución, tras celebrar consultas con los representantes designados por los grupos políticos con representación parlamentaria, vengo en proponer al Excmo. señor don Pedro Sánchez Pérez-Castejón, como candidato a la Presidencia del Gobierno. Lo que comunico a V.E. para que se formule al Congreso de los Diputados la oportuna propuesta. Palacio de la Zarzuela, 2 de febrero de 2016. Felipe VI, Rey. El presidente del Congreso de los Diputados, Patxi López Álvarez.'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no RN', 218),\n",
       " ('ser VSIP3S0', 217),\n",
       " ('señoría NCFP000', 191),\n",
       " ('gobierno NP00O00', 111),\n",
       " ('más RG', 92),\n",
       " ('acuerdo NCMS000', 73),\n",
       " ('españa NP00G00', 70),\n",
       " ('cambio NCMS000', 62),\n",
       " ('también RG', 60),\n",
       " ('español NCMP000', 58),\n",
       " ('haber VAIP3S0', 49),\n",
       " ('haber VAIP3P0', 48),\n",
       " ('año NCMP000', 47),\n",
       " ('hoy RG', 43),\n",
       " ('aplausos NP00G00', 42),\n",
       " ('hacer VMN0000', 38),\n",
       " ('diálogo NCMS000', 38),\n",
       " ('haber VAIP1P0', 36),\n",
       " ('cámara NP00O00', 35),\n",
       " ('estado NP00O00', 35)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filteredFreelings = filterStopwordsViaPosTagCategory(freelings, postTagsCategoryExcluded = [postagConjunction, postagDeterminer, postagPronoun, postagAdposition, postagPunctuation])\n",
    "lemasAndPoS = [f.lema + ' ' + f.postag for f in filteredFreelings]\n",
    "\n",
    "bagOfWords = Counter(lemasAndPoS)\n",
    "bagOfWords.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3458"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords = len(bagOfWords)\n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06304222093695779"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWords['no RN'] / uniqueWords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
